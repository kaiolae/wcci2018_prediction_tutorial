{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying temporal data with non temporal models\n",
    "\n",
    "In this jupyter notebook we will classify human acitivities from accelerometer data using a Random Forest classifier. We will be using the human activity recognition WISDM dataset (http://www.cis.fordham.edu/wisdm/dataset.php) which contains 6 different activities: walking, jogging, upstairs, downstairs, sitting, standing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from scipy.io import arff\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define global variables\n",
    "filename = \"wisdm_modified.arff\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data with features extracted\n",
    "The .arff file contains 43 extracted features from the raw accelerometer data. We will load the .arff file and convert it into a pandas data frame. Again, we will do data exploration but this time on the extracted features.\n",
    "\n",
    "<img src=\"img/exploration.png\" width=\"100\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load arff file.\n",
    "dataset = arff.loadarff(filename)\n",
    "dataset = pd.DataFrame(dataset[0])\n",
    "\n",
    "# remove double quotes in column names\n",
    "dataset.columns = dataset.columns.str.replace('\\\"','')\n",
    "\n",
    "#print the frist data frame rows\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows with nan. We may want to impute those values instead.\n",
    "dataset = dataset.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#compute summary statistics\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset[['XSTANDDEV','class']].groupby('class').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The same but with sql syntax\n",
    "from pandasql import sqldf\n",
    "\n",
    "#initialize\n",
    "pysqldf = lambda q: sqldf(q, globals())\n",
    "\n",
    "print(pysqldf(\"\"\"SELECT class, COUNT(class), AVG(XSTANDDEV), MIN(XSTANDDEV), MAX(XSTANDDEV) FROM dataset GROUP BY class;\"\"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/preprocessing.png\" width=\"100\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unused columns\n",
    "dataset = dataset.drop(['UNIQUE_ID','user','XAVG'], axis=1) #XAVG is all zeros\n",
    "\n",
    "#Convert bytes to string\n",
    "dataset['class'] = dataset['class'].str.decode(\"utf-8\")\n",
    "\n",
    "#shuffle the rows\n",
    "seed = 321 #set seed for reproducibility\n",
    "np.random.seed(seed)\n",
    "\n",
    "dataset = shuffle(dataset) # In some cases, it is a good practice to shuffle the data\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select features and class\n",
    "features = dataset.drop(['class'], axis=1)\n",
    "labels = dataset[['class']]\n",
    "\n",
    "#convert to numpy array\n",
    "features = features.values\n",
    "labels = labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels converting them from strings to integers\n",
    "le = LabelEncoder()\n",
    "labels_int = le.fit_transform(labels.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first labels\n",
    "labels_int[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tt.png\" width=\"100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-fold cross validation\n",
    "\n",
    "Commonly used to estimate the generalization performance of a predictive model. Divide the dataset into k subsets. Perform k iterations. In each iteration take one of the subsets and use it as the test set. Use the remaining subsets as the train set. Each subset is used as test set once and only once. Stratified cross validation preserves the percentage of samples for each class (this is what we will use here).\n",
    "\n",
    "![title](img/kfold.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our cross validation strategy.\n",
    "skf = StratifiedKFold(n_splits=10, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/48687375/deprecation-error-in-sklearn-about-empty-array-without-any-empty-array-in-my-cod?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa\n",
    "warnings.filterwarnings('ignore') #supress warnings from scikit learn\n",
    "\n",
    "#variables to accumulate predictions in int format\n",
    "acum_true_classes_int = np.empty((0,))\n",
    "acum_predicted_classes_int = np.empty((0,))\n",
    "\n",
    "#variables to accumulate predictions in string format\n",
    "acum_true_classes = np.empty((0,))\n",
    "acum_predicted_classes = np.empty((0,))\n",
    "\n",
    "\n",
    "i = 0\n",
    "for train_idxs, test_idxs in skf.split(features, labels_int):\n",
    "    i = i + 1\n",
    "    print(\"=================Fold \", i, \"/\", 10)\n",
    "    clf = RandomForestClassifier(n_estimators = 100, random_state=seed)\n",
    "    \n",
    "    # Normalize features between 0 and 1\n",
    "    # This is done within each fold and normalization parameters learned just from the training data\n",
    "    normalizer = preprocessing.MinMaxScaler().fit(features[train_idxs,])\n",
    "    train_normalized = normalizer.transform(features[train_idxs,])\n",
    "    test_normalized = normalizer.transform(features[test_idxs])\n",
    "    \n",
    "    #train classifier with the training data\n",
    "    clf.fit(train_normalized, labels_int[train_idxs])\n",
    "    predictions = clf.predict(test_normalized)\n",
    "    \n",
    "    acum_true_classes_int = np.hstack((acum_true_classes_int, labels_int[test_idxs]))\n",
    "    acum_predicted_classes_int = np.hstack((acum_predicted_classes_int, predictions))\n",
    "    \n",
    "    # convert classes back to strings\n",
    "    true_classes = le.inverse_transform(labels_int[test_idxs])\n",
    "    predicted_classes = le.inverse_transform(predictions)\n",
    "    acum_true_classes = np.hstack((acum_true_classes, true_classes))\n",
    "    acum_predicted_classes = np.hstack((acum_predicted_classes, predicted_classes))\n",
    "    \n",
    "    \n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/results.png\" width=\"100\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "pd.crosstab(acum_true_classes, acum_predicted_classes, rownames=['True labels'], colnames=['Predicted labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance metrics\n",
    "\n",
    "P: the number of total positive cases  \n",
    "N: the number of total negative cases  \n",
    "TP: number of correctly classified positive samples  \n",
    "TN: number of correctly classified negative samples  \n",
    "FP: number of negative samples incorrectly classified as positive  \n",
    "FN: number of positive samples incorrectly classified as negative  \n",
    "\n",
    "\n",
    "\n",
    "### Accuracy\n",
    "Percentage of correctly classified instances.\n",
    "$$ACC=\\frac{TP + TN}{P + N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(acum_true_classes_int, acum_predicted_classes_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall (sensitivity)\n",
    "The proportion of positives that are correctly identified as such.\n",
    "\n",
    "$$RECALL=\\frac{TP}{P}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average='macro' will report the average across all classes.\n",
    "#average=None will report the performance metric for each class.\n",
    "recall_score(acum_true_classes_int, acum_predicted_classes_int, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision (positive predictive value)\n",
    "The ability of the classifier not to label as positive a sample\n",
    "that is negative. Equivalently, it is the fraction of relevant instances among the selected ones.\n",
    "\n",
    "$$PRECISION=\\frac{TP}{TP + FP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(acum_true_classes_int, acum_predicted_classes_int, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](img/PrecisionRecall.png)\n",
    "By Walber - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=36926283"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "- In this part of the tutorial we perfomred activity recognition from sensor data.  \n",
    "- We learned how to use python to explore the data and pre-process it.\n",
    "- We used scikit-learn to train a Random Forest classifier and achieved an acceptable performance.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
