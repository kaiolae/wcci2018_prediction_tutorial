{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level RNN using LSTM cells.\n",
    "\n",
    "- Trains on Star Trek episode titles\n",
    "- Outputs \"fake\" titles.\n",
    "\n",
    "Much comes from a [Keras example](https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py).\n",
    "\n",
    "## Setup Environment\n",
    "\n",
    "- Import Keras\n",
    "- Open up the Star Trek corpus\n",
    "- We need to translate the textual data into a format that the RNN can accept as input.\n",
    "- Give each letter an index and create dictionaries to translate from index to character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 11017\n",
      "total chars: 52\n",
      "Max: 50\n",
      "Mean: 14.010899182561309\n",
      "Median: 13.0\n",
      "Min: 2\n",
      "Character Dictionary:  {'\\n': 0, ' ': 1, '!': 2, \"'\": 3, '(': 4, ')': 5, ',': 6, '-': 7, '.': 8, '0': 9, '1': 10, '2': 11, '3': 12, '4': 13, '5': 14, '7': 15, '8': 16, '9': 17, ':': 18, '?': 19, 'a': 20, 'b': 21, 'c': 22, 'd': 23, 'e': 24, 'f': 25, 'g': 26, 'h': 27, 'i': 28, 'j': 29, 'k': 30, 'l': 31, 'm': 32, 'n': 33, 'o': 34, 'p': 35, 'q': 36, 'r': 37, 's': 38, 't': 39, 'u': 40, 'v': 41, 'w': 42, 'x': 43, 'y': 44, 'z': 45, '\\xa0': 46, '©': 47, 'â': 48, 'ã': 49, '€': 50, '™': 51}\n",
      "Inverse Character Dictionary:  {0: '\\n', 1: ' ', 2: '!', 3: \"'\", 4: '(', 5: ')', 6: ',', 7: '-', 8: '.', 9: '0', 10: '1', 11: '2', 12: '3', 13: '4', 14: '5', 15: '7', 16: '8', 17: '9', 18: ':', 19: '?', 20: 'a', 21: 'b', 22: 'c', 23: 'd', 24: 'e', 25: 'f', 26: 'g', 27: 'h', 28: 'i', 29: 'j', 30: 'k', 31: 'l', 32: 'm', 33: 'n', 34: 'o', 35: 'p', 36: 'q', 37: 'r', 38: 's', 39: 't', 40: 'u', 41: 'v', 42: 'w', 43: 'x', 44: 'y', 45: 'z', 46: '\\xa0', 47: '©', 48: 'â', 49: 'ã', 50: '€', 51: '™'}\n"
     ]
    }
   ],
   "source": [
    "## Much borrowed from https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import load_model\n",
    "\n",
    "from keras.callbacks import LambdaCallback\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "#Helper method sampling and generating text from an RNN after training\n",
    "from SamplingAndGeneratingText import generate_text_segment\n",
    "\n",
    "text = open(\"startrekepisodes.txt\").read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocabulary_size = len(chars)\n",
    "print('total chars:', vocabulary_size)\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "\n",
    "# How long is a title?\n",
    "titles = text.split('\\n')\n",
    "lengths = np.array([len(n) for n in titles])\n",
    "print(\"Max:\", np.max(lengths))\n",
    "print(\"Mean:\", np.mean(lengths))\n",
    "print(\"Median:\", np.median(lengths))\n",
    "print(\"Min:\", np.min(lengths))\n",
    "\n",
    "# hence choose 30 as seuence length to train on.\n",
    "print(\"Character Dictionary: \", char_indices)\n",
    "print(\"Inverse Character Dictionary: \", indices_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training Data\n",
    "\n",
    "- Cut up the corpus into semi-redundant sequences of 30 characters.\n",
    "- Change indices into \"one-hot\" vector encodings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/slicing_text.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 3663\n",
      "the man trap\n",
      "charlie x\n",
      "where n\n",
      "o\n"
     ]
    }
   ],
   "source": [
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 30\n",
    "step = 3\n",
    "\n",
    "sentences = [] #The training data\n",
    "next_chars = [] #The training labels\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "    \n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print(sentences[0])\n",
    "print(next_chars[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Onehot encoding:\n",
    "* a -> [1, 0, 0, ..., 0]\n",
    "* b -> [0, 1, 0, ..., 0]\n",
    "* ...\n",
    "\n",
    "Each training sample becomes 2D tensor:\n",
    "* \"This is the text\" -> X = [[0, 0, ..., 1, 0, ..., 0], ..., [0, 0, ..., 1, 0, ... 0]]\n",
    "\n",
    "Each target (next letter) becomes 1D onehot tensor:\n",
    "* a -> y = [1, 0, 0, ..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preparing training corpus, shapes of sets are:\n",
      "X shape: (3663, 30, 52)\n",
      "y shape: (3663, 52)\n",
      "Vocabulary of characters: 52\n"
     ]
    }
   ],
   "source": [
    "#X shape: 3D tensor. First dimension is the sentences, second is each letter in each sentence, third is the onehot\n",
    "#vector representing that letter.\n",
    "X = np.zeros((len(sentences), maxlen, vocabulary_size), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), vocabulary_size), dtype=np.bool)\n",
    "    \n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    \n",
    "print(\"Done preparing training corpus, shapes of sets are:\")\n",
    "print(\"X shape: \" + str(X.shape))\n",
    "print(\"y shape: \" + str(y.shape))\n",
    "print(\"Vocabulary of characters:\", vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- Model has one hidden layer of 128 LSTM cells.\n",
    "- Output layer uses the \"softmax\" activation function to output a probability distribution over next letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/n-in-1-out.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layer_size = 128\n",
    "model_train = Sequential()\n",
    "model_train.add(LSTM(layer_size, input_shape=(maxlen, len(chars))))\n",
    "# Project back to vocabulary. One output node for each letter.\n",
    "# Dense indicates a fully connected layer.\n",
    "# Softmax activation ensures the combined values of all outputs form a probability distribution:\n",
    "# They sum to 1, with each individual value between 0 and 1.\n",
    "model_train.add(Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               92672     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 52)                6708      \n",
      "=================================================================\n",
      "Total params: 99,380\n",
      "Trainable params: 99,380\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Categorical crossentropy  minimizes the distance between the probability distributions \n",
    "# output by the network and the true distribution of the targets.\n",
    "# The optimizer specifies HOW the gradient of the loss will be used to update parameters.\n",
    "# Different optimizers have different tricks to avoid local optima, etc.\n",
    "# RMSProp is adaptive, adjusting the rate of learning to how fast we're currently learning.\n",
    "model_train.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=0.01))\n",
    "model_train.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "- Train on batches of 128 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callback, which starts some text generation after each epoch.\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    diversity = 0.5 #Can be modified to change the amount of creativity in the network\n",
    "\n",
    "    generated = generate_text_segment(text, 400, diversity, model_train,\n",
    "                                      maxlen, len(chars), char_indices, indices_char)\n",
    "    sys.stdout.write(generated)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"cy\n",
      "the void\n",
      "workforce, part i\n",
      "\"cy\n",
      "the void\n",
      "workforce, part i\n",
      "re te serererte\n",
      "r\n",
      " noe\n",
      "\n",
      "trdioonhieno\n",
      "\n",
      "\n",
      "t ei\n",
      "sre rho r\n",
      "pori\n",
      "\n",
      "ilrs\n",
      "fritr\n",
      "eaai\n",
      "rie\n",
      "\n",
      "rab iii\n",
      "t\n",
      "orarrreliiaheeio\n",
      "rrer\n",
      "ee\n",
      "\n",
      "eeieii tntooacaoio o ec\n",
      "lo\n",
      "\n",
      "\n",
      "irr \n",
      "is\n",
      "f iorhearre\n",
      "tan\n",
      "\n",
      "ro\n",
      "\n",
      "la\n",
      "nrei\n",
      "\n",
      "naalitrih\n",
      "rethtrmrire hrea n toioror osociil aoiaceapl\n",
      "eaga\n",
      "a\n",
      "o\n",
      "li\n",
      "a\n",
      "m raaer y ani\n",
      "le\n",
      "\n",
      "oo hehre\n",
      "\n",
      "rh io\n",
      "an\n",
      "\n",
      "ni\n",
      "otebt\n",
      "iip rre\n",
      "iraiet\n",
      " ili\n",
      " te sorioo\n",
      "\n",
      "\n",
      "eeet\n",
      "i\n",
      " at\n",
      "oirrr \n",
      "oroairs it htgr\n",
      "a  t\n",
      "ihee\n",
      "a\n",
      "r s\n",
      "ioe \n",
      "rti\n",
      "eieohi o\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \"ge ã  troi\n",
      "transfigurations\n",
      "th\"ge ã  troi\n",
      "transfigurations\n",
      "the minn thnti tho olonal\n",
      "t n\n",
      "goon  anons oncthe oib\n",
      "the lol  he ala che mael n ce lng laraniay nan panl pgt osnou ogatovini ldlincon\n",
      "\n",
      "t g thins\n",
      "the bancton nann\n",
      "nonleroi\n",
      "thonc le thin anthon  pnsondent nigewn \n",
      "the tanron ane lnd  ionl\n",
      "nngnsino t nrg ininsl la angignnn\n",
      "ana oan\n",
      "fonnn she\n",
      "the thinn singn\n",
      "nart s saring lf ton ll llor lor al  an  aathe llnl thaln nn  mos sale cnne llln\n",
      "alonns\n",
      "the the ma\n",
      "\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"n\n",
      "impulse\n",
      "exile\n",
      "the shipment\n",
      "t\"n\n",
      "impulse\n",
      "exile\n",
      "the shipment\n",
      "the sirescire deart parthe sibrtis armtce int pirtisise the she the the the bett rerthe eali\n",
      "the the she the sierthe the the f the serthe\n",
      "the siant\n",
      "uris\n",
      "the the the siistriinthe the therthe berthe the the of the sart iatte shi the marticthe she the siorot ortirt or bins the the wert an\n",
      "sirs\n",
      "the she the shiss or the the the nige the bart ort ont gart ect eige mertist onors\n",
      "parthe farthe tastil she t\n",
      "\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"i\n",
      "storm front, part ii\n",
      "home\n",
      "bo\"i\n",
      "storm front, part ii\n",
      "home\n",
      "boont ing\n",
      "the ne boof on the se\n",
      "the the te the be s\n",
      " o  of port do t\n",
      "ol\n",
      "the sis\n",
      "the sare oo the pangor\n",
      "d\n",
      "the cori f so\n",
      "the sale\n",
      "toond ce wopare de fof sion\n",
      "the s\n",
      "part o\n",
      "s\n",
      "so paod go the wa the of pe the s of on coon\n",
      "the the sore\n",
      "the part on\n",
      "the co tho not\n",
      "ocor\n",
      "the ane\n",
      "the se se on ane des\n",
      "sas of fare\n",
      "si on the mene\n",
      "teoo\n",
      "the tiond ge the wemon\n",
      "the co fore sos an pepcone oo the soge ant pesngos\n",
      "se par\n",
      "\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"\n",
      "dead stop\n",
      "a night in sickbay\n",
      "\"\n",
      "dead stop\n",
      "a night in sickbay\n",
      "and of arion\n",
      "in corad part one\n",
      "ae of and on are of aring\n",
      "the and\n",
      "deart pirt\n",
      "foronc\n",
      "sare sare in\n",
      "darithe fart or\n",
      "are the on ont an arh ind\n",
      "ware\n",
      "sharo\n",
      "the ant an of eart\n",
      "oqior of anid\n",
      "part i\n",
      "are of ariin\n",
      "\n",
      "iathe iof oni of and of altir\n",
      "sis\n",
      "the heare\n",
      "domont ind\n",
      "of and\n",
      "ine\n",
      "dars\n",
      "inge and\n",
      "shend\n",
      "enid\n",
      "she amond\n",
      "bare oof of hoas\n",
      "one of are the cool ini\n",
      "daad of ariti\n",
      "the dare ine\n",
      "shese\n",
      "she mar\n",
      "the of are\n",
      "che\n",
      "\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \"moon\n",
      "prodigal daughter\n",
      "latent \"moon\n",
      "prodigal daughter\n",
      "latent sart on the court in the fort hes\n",
      "the seart\n",
      "encors of hagh sort datt se\n",
      "the sion\n",
      "the shant\n",
      "the seant and sors of the she sorin, part s\n",
      "the seart\n",
      "the part in\n",
      "the shaly\n",
      "the ant part on of hete the derolishe shal\n",
      "the sand bothe pant on are sigeting\n",
      "the enant tre she shors\n",
      "and the shasale shess\n",
      "the part s\n",
      "the sce dese part on part on fond of the mess of the the che the bont the sare ant the part the s\n",
      "\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"rt\n",
      "heart of stone\n",
      "phage\n",
      "the cl\"rt\n",
      "heart of stone\n",
      "phage\n",
      "the clige ing the shorn\n",
      "the ssor hole\n",
      "the shond\n",
      "the monat or on the serol se part oo\n",
      "the shing\n",
      "the bage of artion\n",
      "the mont in arone of tire\n",
      "the page of antori\n",
      "the moge of part io\n",
      "era part on\n",
      "the s part io\n",
      "the bong\n",
      "the sher in dorct on part ioran parn of and the comens of hore gore (part ii\n",
      "the moncol of the lese of art ore ant gounci\n",
      "the part of andore\n",
      "soriay\n",
      "perite of a the song of corne for the dopris\n",
      "\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"s\n",
      "terra prime\n",
      "these are the vo\"s\n",
      "terra prime\n",
      "these are the vors\n",
      "the worktisce\n",
      "shashore\n",
      "the arhis\n",
      "the shari\n",
      "the shir\n",
      "the shill\n",
      "ore part ii\n",
      "the the shari\n",
      "the the borktiration\n",
      "the his\n",
      "shishitio\n",
      "shiproshin\n",
      "vite of sion\n",
      "the chus\n",
      "ine andick\n",
      "the shor\n",
      "time sire\n",
      "the hithe of h trith\n",
      "the shtros\n",
      "aach\n",
      "tie the gholl\n",
      "the shide\n",
      "the vosss core the shal of thi hist\n",
      "werol and chilisits\n",
      "the shigal\n",
      "the arili\n",
      "ghivis\n",
      "sholl\n",
      "she sorint\n",
      "made vist\n",
      "the mart in\n",
      "the kithe shorcico\n",
      "shad\n",
      "\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \" society\n",
      "conundrum\n",
      "power play\n",
      "\" society\n",
      "conundrum\n",
      "power play\n",
      "the sere\n",
      "sure\n",
      "the sige in the shear\n",
      "the sige in the and the begoldent of the searitive\n",
      "shen of the dart of the seart of the sion\n",
      "propayion, part ois\n",
      "the shence\n",
      "the of the seatsof part io\n",
      "the bonce\n",
      "cortion\n",
      "the haye\n",
      "the mendle cougeting the shincof fort on the bige of the sion\n",
      "the hest\n",
      "the the fere of tine\n",
      "the syert on of the the forke gare of the sige and of the s ancoro of the sion\n",
      "the wary\n",
      "the si\n",
      "\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"he house of quark\n",
      "equilibrium\n",
      "\"he house of quark\n",
      "equilibrium\n",
      "neart iingive besinging\n",
      "the boudly\n",
      "ambith sunging surnating cousting of the shartion\n",
      "coldly\n",
      "ar und the diting the sharking\n",
      "the  hild\n",
      "end the langong\n",
      "bablling\n",
      "the sharfiine suarnd\n",
      "the sindens\n",
      "the shigring\n",
      "sulemant the meninging\n",
      "surenting\n",
      "the shirning\n",
      "suanting the mander\n",
      "the fouge the part i\n",
      "the shadlore\n",
      "the fimling\n",
      "surce\n",
      "surntar\n",
      "the blodlanger of halline\n",
      "the mand\n",
      "bount and part i\n",
      "the warlt in and t\n",
      "\n",
      "----- Generating text after Epoch: 10\n",
      "----- Generating with seed: \"nae\n",
      "duet\n",
      "timescape\n",
      "in the hand\"nae\n",
      "duet\n",
      "timescape\n",
      "in the handing\n",
      "the silchor arightis\n",
      "pars and chill\n",
      "bive fine\n",
      "list of emanting\n",
      "surinition, part ii\n",
      "the pissigan of kill\n",
      "eming\n",
      "the singil starthis stint\n",
      "coutt caks\n",
      "dage fiont\n",
      "the foist part ii\n",
      "the ponsisurice\n",
      "shipact or emond\n",
      "the silist\n",
      "cring\n",
      "the forst one the shill\n",
      "en of holle in the blint\n",
      "the fore part\n",
      "the singil of fored of etingal of hell\n",
      "the dok udle sor\n",
      "chiokinger fronting\n",
      "the sings part ii\n",
      "the wall\n",
      "sick\n",
      "\n",
      "----- Generating text after Epoch: 11\n",
      "----- Generating with seed: \"glory\n",
      "worst case scenario\n",
      "empo\"glory\n",
      "worst case scenario\n",
      "empone the bltact\n",
      "ex\n",
      "shad ok the shact\n",
      "the mega of halle and the shar, part ii\n",
      "the dest of the seacicos and the sigany porth ore vill\n",
      "the the nold\n",
      "the hama\n",
      "the sary\n",
      "emarnt\n",
      "the past the peact\n",
      "doss pant ons\n",
      "comssignt\n",
      "cant on a the sear\n",
      "tict on chis\n",
      "hes of the sian\n",
      "tin ss part ii\n",
      "the stach of magal\n",
      "seressict\n",
      "the meally\n",
      "ex\n",
      "mes of home\n",
      "the denacition\n",
      "corday and of the seary\n",
      "inact one pare of hally\n",
      "the sian\n",
      "\n",
      "----- Generating text after Epoch: 12\n",
      "----- Generating with seed: \"(part 1)\n",
      "heroes and demons\n",
      "cat\"(part 1)\n",
      "heroes and demons\n",
      "cattre sire\n",
      "inf the gale\n",
      "io are\n",
      "coustice\n",
      "surre of tort\n",
      "iore fire\n",
      "bowe part ii\n",
      "the dessire\n",
      "okerles of a tione\n",
      "the donge part oie\n",
      "allod of hole\n",
      "erile of rattion\n",
      "the searntion\n",
      "the werades\n",
      "werolions part ii\n",
      "the searn\n",
      "oreal of holine\n",
      "bart ii\n",
      "the allederter\n",
      "weronion\n",
      "the searndor\n",
      "the count\n",
      "tore and dorthols\n",
      "allesion\n",
      "redile\n",
      "andorkion, part oie\n",
      "the sisy\n",
      "cerolions farndion\n",
      "the holles\n",
      "the dearnor\n",
      "the neanter\n",
      "th\n",
      "\n",
      "----- Generating text after Epoch: 13\n",
      "----- Generating with seed: \"ne little ship\n",
      "retrospect\n",
      "hono\"ne little ship\n",
      "retrospect\n",
      "honos\n",
      "the imentiv donssinger desthe frond\n",
      "the warndyes\n",
      "list one the wearndingengerinity\n",
      "the partlise\n",
      "barilise\n",
      "fart oie and the sise\n",
      "wurnationss ar pistion: part ii\n",
      "the douglly\n",
      "the upinger dearndine\n",
      "blles the firgt descoss gart of hold\n",
      "ementions part ii\n",
      "thi the sirges of holise\n",
      "bame bive and the bougt intive and the wory\n",
      "besir\n",
      "setprite of a thil\n",
      "serper of holle sharban the siadion\n",
      "retime\n",
      "the sigily\n",
      "bul\n",
      "\n",
      "----- Generating text after Epoch: 14\n",
      "----- Generating with seed: \"ture\n",
      "the darkness and the ligh\"ture\n",
      "the darkness and the light\n",
      "urqok\n",
      "e rend, part ii\n",
      "the sion\n",
      "the shand\n",
      "the pangenancor\n",
      "the konge fart i\n",
      "comekion\n",
      "lite\n",
      "carnanct\n",
      "marn for the warciin, part ii\n",
      "the mond for the sharchion\n",
      "reables farct\n",
      "worsarn, part i\n",
      "the dearnd\n",
      "farstictr\n",
      "the pars of gomant\n",
      "the the destregr af the voss\n",
      "demole of hole\n",
      "the forst fart two\n",
      "romation\n",
      "the warcecof attr\n",
      "the sqarce\n",
      "surpromancok\n",
      "ance fart io\n",
      "s mpart owe\n",
      "the fondt fart io\n",
      "slos's part ii\n",
      "th\n",
      "\n",
      "----- Generating text after Epoch: 15\n",
      "----- Generating with seed: \"persistence of vision\n",
      "rejoined\"persistence of vision\n",
      "rejoined\n",
      "the the ghods\n",
      "the hagh or the degaly\n",
      "entingturn\n",
      "the spary\n",
      "warco\n",
      "the megr of the blont the brog fist\n",
      "crment\n",
      "rutring\n",
      "lodanttrne\n",
      "loss mage\n",
      "bougt\n",
      "trint on's proth ore\n",
      "the the ghodre ar tine\n",
      "the the hard\n",
      "emies\n",
      "the shager\n",
      "the light\n",
      "the moge fart i\n",
      "the sigry\n",
      "the forgt gartor\n",
      "shile\n",
      "supret ok for tho the blond of fore\n",
      "the fart on\n",
      "the the the the hear\n",
      "the shanger werp of the hear\n",
      "the shanger of the frogklo\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Generating text after Epoch: 16\n",
      "----- Generating with seed: \"ity of life\n",
      "chain of command: \"ity of life\n",
      "chain of command: fart owe\n",
      "retimes\n",
      "ritries\n",
      "resilishadays and io hap a foint\n",
      "carcof fougt of aphes\n",
      "chisterant cormontirn: part i\n",
      "\n",
      "the ways arore of a the shad\n",
      "ous part owe\n",
      "resens\n",
      "the sianit: part io\n",
      "eminding chist\n",
      "the waycecance fart wor\n",
      "arlolisur\n",
      "balile\n",
      "shockanger anovill\n",
      "angacaco\n",
      "the fouga in a the searerine bearp's stige of apcotie chish\n",
      "warcoint\n",
      "trealy\n",
      "enceca aact ii\n",
      "the ways ar ii\n",
      "the siagerine\n",
      "butimes\n",
      "the waya\n",
      "\n",
      "----- Generating text after Epoch: 17\n",
      "----- Generating with seed: \"ra\n",
      "dark frontier, part i\n",
      "dark \"ra\n",
      "dark frontier, part i\n",
      "dark pronter of holle\n",
      "shellention\n",
      "remendiol\n",
      "propaction\n",
      "brite on the ome pist\n",
      "the flogh or prosters\n",
      "the sursion\n",
      "reakine\n",
      "balole\n",
      "brobble of auma\n",
      "the saye\n",
      "fougt of apmane\n",
      "the blent\n",
      "urearor's pre for the sear\n",
      "the way of holle of ampar\n",
      "the moundion\n",
      "rustion: part owe\n",
      "reminitions\n",
      "lollles the waycof halles\n",
      "the waycon prector\n",
      "emorkion\n",
      "rutte shedper of holle of a mar\n",
      "the soraloloshe of holo\n",
      "urise\n",
      "brokle\n",
      "brokless\n",
      "\n",
      "\n",
      "----- Generating text after Epoch: 18\n",
      "----- Generating with seed: \"pegasus\n",
      "homeward\n",
      "armageddon ga\"pegasus\n",
      "homeward\n",
      "armageddon gang\n",
      "sarner of the bol the blagh syossarions\n",
      "day of digat ois andidagator\n",
      "sunden of mogr deary\n",
      "io arna worruvive shalleny\n",
      "the the firgt ....\n",
      "1alealor, part i\n",
      "the worcepar the siaght: part owe\n",
      "ritime\n",
      "shol of the beaged\n",
      "en in tint frond\n",
      "sens our pestion: part ii\n",
      "chibleavion: part i\n",
      "dous ant intittes\n",
      "chish surr, part i\n",
      "the wirsiag, part i\n",
      "doutless\n",
      "the fingaly\n",
      "remonaviviss\n",
      "sholsss ar the sior\n",
      "ex of a mo\n",
      "\n",
      "----- Generating text after Epoch: 19\n",
      "----- Generating with seed: \"est of both worlds: part one\n",
      "t\"est of both worlds: part one\n",
      "the wishter\n",
      "werbabone pearnding coortticn\n",
      "shodame hon parndion\n",
      "bllay one\n",
      "the sime\n",
      "the fimge batt re\n",
      "the wayconce of homo\n",
      "the thelll\n",
      "propabldion\n",
      "rivilico\n",
      "baboli\n",
      "bowe tiont\n",
      "the pact\n",
      "encornacy\n",
      "anganctyane\n",
      "bountd part oi\n",
      "the wommes of the leay\n",
      "shild's part ow\n",
      "the wishil\n",
      "\n",
      "arfict\n",
      "weron\n",
      "io firituon: part ii\n",
      "hillles of holle\n",
      "shapant frist\n",
      "sarcok one the vililote cattist\n",
      "carclengs of the mork\n",
      "prongr part\n",
      "in\n",
      "Training done\n"
     ]
    }
   ],
   "source": [
    "# Training the Model. history captures data for plotting (e.g loss)\n",
    "print(\"training start\")\n",
    "#Setting up a callback, which will generate example text from the network during training.\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "history = model_train.fit(X, y, batch_size=128, epochs=20, verbose=0, callbacks=[print_callback])\n",
    "print(\"Training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model if necessary\n",
    "model_train.save(\"keras-startrek-LSTM-model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting training and validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ab0860f13918>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mhistory_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mloss_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_values\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss']\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'b-o', label='Training loss')\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
